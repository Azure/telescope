trigger: none

variables:
  # Hardcoded ADO pipeline variables
  AZURE_SUBSCRIPTION_ID: "37deca37-c375-4a14-b90a-043849bd2bf1"
  SKIP_RESOURCE_MANAGEMENT: "false"
  AZURE_SERVICE_CONNECTION: "Azure-for-Telescope-internal"
  AZURE_STORAGE_ACCOUNT_NAME: "akstelescope"
  AZURE_TELESCOPE_STORAGE_ACCOUNT_NAME: "telescopedata"
  LOCATION: "uksouth"
  CREATESWIFTV2PING: "true"
  CLEANUP_RESOURCES: "true"
  K8S_VERSION: "1.33"
  PROVISION_BUFFER_NODES: "false"
  # Log Analytics are expensive - disable and delete after use
  # WARNING: Enabling Log Analytics adds +1 DaemonSet pod per node - increase max_pods by 1 if enabled
  ENABLE_LOG_ANALYTICS: "false"
  # Existing scenario variables
  SCENARIO_TYPE: perf-eval
  SCENARIO_NAME: swiftv2-cluster-churn-feature
  OWNER: aks
  VM_SKU: Standard_D16s_v4
  # count of swiftv2 pods/NIC per step
  PODS_PER_NODE: 7
  DEVICE_PLUGIN: "false"
  PROBE_TARGET_URL: "http://172.27.0.30/"
  PROBE_TIMEOUT: "300"
  
  # Container images - Using centralized ACR (acndev)
  DATAPATH_REPORTER_IMAGE: "acndev.azurecr.io/datapath-reporter:2026.01.05.01"
  NGINX_IMAGE: "acndev.azurecr.io/nginx:latest"
  
  # Image pre-pull batching configuration
  IMAGE_PREPULL_BATCH_SIZE: "100"

  # CL2 parameters
  cpu_per_node: 16
  # max_pods = PODS_PER_NODE + default k8s system pods (5) + log analytics pod (1 if ENABLE_LOG_ANALYTICS=true)
  # Current: 7 + 5 = 12 (increase to 13 if enabling Log Analytics)
  max_pods: 12
  # Common matrix parameters
  repeats: 1
  node_label: "swiftv2slo=true"
  cilium_enabled: False
  scrape_containerd: False
  service_test: False
  ds_test: True

stages:
  - stage: generate_base_run_id
    displayName: 'Generate Base Run ID'
    jobs:
    - job: generate_run_id
      displayName: 'Generate Base Run ID'
      steps:
      - script: |
          # Generate base run ID: Initials-DDHHMMSS (17 chars total with job suffix)
          timestamp=$(date +%d%H%M%S)
          
          # Extract user initials from Build.RequestedFor (e.g., "John Doe" -> "JD")
          user_name="$(Build.RequestedFor)"
          echo "Pipeline triggered by: $user_name"
          
          # Extract initials - get first letter of each word, convert to uppercase, take first 2
          user_initials=$(echo "$user_name" | sed 's/[^A-Za-z ]//g' | awk '{for(i=1;i<=NF;i++) printf substr(toupper($i),1,1)}' | cut -c1-2)
          
          # Fallback if initials are empty or too short
          if [ -z "$user_initials" ] || [ ${#user_initials} -lt 2 ]; then
            user_initials="XX"
          fi
          
          base_run_id="${user_initials}-${timestamp}"
          echo "Generated timestamp: $timestamp"
          echo "User initials: $user_initials"
          echo "Generated base run ID: $base_run_id"
          echo "##vso[task.setvariable variable=BASE_RUN_ID;isOutput=true]$base_run_id"
        name: generate_id
        displayName: "Generate Base Run ID"

  - stage: staticres_gradual
    condition: succeeded()
    displayName: 'Static Gradual'
    dependsOn: 
      - generate_base_run_id
    variables:
      BASE_RUN_ID: $[ stageDependencies.generate_base_run_id.generate_run_id.outputs['generate_id.BASE_RUN_ID'] ]
    jobs:
      - template: /pipelines/system/matrices/swiftv2-staticres-gradual-matrix.yml
        parameters:
          cloud: azure
          regions:
            - $(LOCATION)
          engine: clusterloader2
          engine_input:
            image: "ghcr.io/azure/clusterloader2:v20250311"
          topology: swiftv2
          max_parallel: 1
          timeout_in_minutes: 2160
          credential_type: service_connection
          ssh_key_enabled: false
          base_run_id: $(BASE_RUN_ID)

  - stage: dynamicres_gradual
    condition: succeededOrFailed()
    displayName: 'Dynamic Gradual'
    dependsOn: 
      - generate_base_run_id
      - staticres_gradual
    variables:
      BASE_RUN_ID: $[ stageDependencies.generate_base_run_id.generate_run_id.outputs['generate_id.BASE_RUN_ID'] ]
    jobs:
      - template: /pipelines/system/matrices/swiftv2-dynamicres-gradual-matrix.yml
        parameters:
          cloud: azure
          regions:
            - $(LOCATION)
          engine: clusterloader2
          engine_input:
            image: "ghcr.io/azure/clusterloader2:v20250311"
          topology: swiftv2
          max_parallel: 1
          timeout_in_minutes: 2160
          credential_type: service_connection
          ssh_key_enabled: false
          base_run_id: $(BASE_RUN_ID)

  - stage: staticres_burst
    condition: false
    displayName: 'Static Burst'
    dependsOn: generate_base_run_id
    variables:
      BASE_RUN_ID: $[ stageDependencies.generate_base_run_id.generate_run_id.outputs['generate_id.BASE_RUN_ID'] ]
    jobs:
      - template: /pipelines/system/matrices/swiftv2-staticres-burst-matrix.yml
        parameters:
          cloud: azure
          regions:
            - $(LOCATION)
          engine: clusterloader2
          engine_input:
            image: "ghcr.io/azure/clusterloader2:v20250311"
          topology: swiftv2
          max_parallel: 1
          timeout_in_minutes: 2160
          credential_type: service_connection
          ssh_key_enabled: false
          base_run_id: $(BASE_RUN_ID)
          
  - stage: dynamicres_burst
    condition: false
    displayName: 'Dynamic Burst'
    dependsOn: generate_base_run_id
    variables:
      BASE_RUN_ID: $[ stageDependencies.generate_base_run_id.generate_run_id.outputs['generate_id.BASE_RUN_ID'] ]
    jobs:
      - template: /pipelines/system/matrices/swiftv2-dynamicres-burst-matrix.yml
        parameters:
          cloud: azure
          regions:
            - $(LOCATION)
          engine: clusterloader2
          engine_input:
            image: "ghcr.io/azure/clusterloader2:v20250311"
          topology: swiftv2
          max_parallel: 1
          timeout_in_minutes: 2160
          credential_type: service_connection
          ssh_key_enabled: false
          base_run_id: $(BASE_RUN_ID)
