parameters:
  - name: cloud
    type: string
    default: ""
  - name: engine_input
    type: object
    default: {}
  - name: region
    type: string

steps:
  # Jumpbox Execution Pipeline

  # Start a shared Bastion tunnel (optional) so subsequent SSH/SCP steps can reuse it.
  - template: /steps/ssh/start-bastion-tunnel.yml
    parameters:
      vm_id: $(JUMPBOX_VM_ID)
      bastion_name: $(BASTION_NAME)
      bastion_resource_group: $(BASTION_RESOURCE_GROUP)
      bastion_local_port: 2222
      condition: ne(variables['JUMPBOX_VM_ID'], '')

  # 1. Prepare Remote Workspace
  - template: /steps/ssh/run-command.yml
    parameters:
      vm_ip: $(JUMPBOX_HOST)
      vm_id: $(JUMPBOX_VM_ID)
      bastion_name: $(BASTION_NAME)
      bastion_resource_group: $(BASTION_RESOURCE_GROUP)
      command: |
        rm -rf '/tmp/telescope-run/run-$(Build.BuildId)'
        mkdir -p '/tmp/telescope-run/run-$(Build.BuildId)/modules/python/clusterloader2/autoscale/results'

  # 2. Upload Code and Environment
  - script: |
      set -eo pipefail

      if [ -z "${SSH_KEY_PATH:-}" ] || [ ! -f "$SSH_KEY_PATH" ]; then
        echo "SSH_KEY_PATH not found" >&2; exit 1
      fi

      use_bastion=false
      if [ -n "${BASTION_NAME:-}" ] && [ -n "${BASTION_RESOURCE_GROUP:-}" ] && [ -n "${JUMPBOX_VM_ID:-}" ]; then
        use_bastion=true
      fi

      ssh_opts=(-i "$SSH_KEY_PATH" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null")
      local_port="${BASTION_TUNNEL_PORT:-2222}"
      if [ "${use_bastion}" = "true" ]; then
        # Prefer a shared tunnel if one was started earlier in the job.
        if [ -n "${BASTION_TUNNEL_PORT:-}" ] && timeout 1 bash -c "</dev/tcp/127.0.0.1/${local_port}" >/dev/null 2>&1; then
          echo "Reusing existing Bastion tunnel on localhost:${local_port}"
        else
          echo "Starting Bastion tunnel for upload..."
          az network bastion tunnel \
            --name "$BASTION_NAME" \
            --resource-group "$BASTION_RESOURCE_GROUP" \
            --target-resource-id "$JUMPBOX_VM_ID" \
            --resource-port 22 \
            --port "$local_port" \
            >/tmp/bastion-tunnel-upload-$$.log 2>&1 &
          tunnel_pid=$!
          trap 'kill ${tunnel_pid} 2>/dev/null || true' EXIT
        fi

        for i in $(seq 1 30); do
          if timeout 1 bash -c "</dev/tcp/127.0.0.1/${local_port}" >/dev/null 2>&1; then
            break
          fi
          sleep 1
        done

        ssh_target="${JUMPBOX_USERNAME:-azureuser}@127.0.0.1"
        scp_opts=("${ssh_opts[@]}" -P "$local_port")
      else
        if [ -z "${JUMPBOX_HOST:-}" ]; then
           echo "JUMPBOX_HOST is not set." >&2
           exit 1
        fi
        ssh_target="${JUMPBOX_USERNAME:-azureuser}@${JUMPBOX_HOST}"
        scp_opts=("${ssh_opts[@]}")
      fi

      # Pack modules
      tar_archive="/tmp/telescope-modules-$(Build.BuildId).tar.gz"
      tar -czf "$tar_archive" -C "$(Pipeline.Workspace)/s" modules/python

      remote_root="/tmp/telescope-run/run-$(Build.BuildId)"
      remote_python_dir="${remote_root}/modules/python"

      # Generate Environment File
      env_file="$(mktemp)"
      {
         echo "export CLOUD=\"${CLOUD}\""
         echo "export REGION=\"${REGION}\""
         echo "export PYTHON_SCRIPT_FILE=\"${remote_python_dir}/clusterloader2/autoscale/autoscale.py\""
         echo "export CL2_IMAGE=\"${CL2_IMAGE}\""
         echo "export CL2_CONFIG_DIR=\"${remote_python_dir}/clusterloader2/autoscale/config\""
         echo "export CL2_REPORT_DIR=\"${remote_python_dir}/clusterloader2/autoscale/results\""
         echo "export CPU_PER_NODE=\"${CPU_PER_NODE}\""
         echo "export NODE_COUNT=\"${NODE_COUNT}\""
         echo "export POD_COUNT=\"${POD_COUNT}\""
         echo "export SCALE_UP_TIMEOUT=\"${SCALE_UP_TIMEOUT}\""
         echo "export SCALE_DOWN_TIMEOUT=\"${SCALE_DOWN_TIMEOUT}\""
         echo "export LOOP_COUNT=\"${LOOP_COUNT}\""
         echo "export NODE_LABEL_SELECTOR=\"${NODE_LABEL_SELECTOR}\""
         echo "export NODE_SELECTOR=\"${NODE_SELECTOR}\""
         echo "export WARMUP_DEPLOYMENT=\"${WARMUP_DEPLOYMENT}\""
         echo "export WARMUP_DEPLOYMENT_TEMPLATE=\"${WARMUP_DEPLOYMENT_TEMPLATE}\""
         echo "export DEPLOYMENT_TEMPLATE=\"${DEPLOYMENT_TEMPLATE}\""
         echo "export OS_TYPE=\"${OS_TYPE}\""
         echo "export POD_CPU_REQUEST=\"${POD_CPU_REQUEST}\""
         echo "export POD_MEMORY_REQUEST=\"${POD_MEMORY_REQUEST}\""
         echo "export CL2_CONFIG_FILE=\"${CL2_CONFIG_FILE}\""
         echo "export ENABLE_PROMETHEUS=\"${ENABLE_PROMETHEUS}\""
         echo "export SCRAPE_KUBELETS=\"${SCRAPE_KUBELETS}\""
         echo "export SCRAPE_KSM=\"${SCRAPE_KSM}\""
         echo "export REMOTE_PYTHON_DIR=\"${remote_python_dir}\""
      } > "$env_file"

      echo "Uploading..."
      scp "${scp_opts[@]}" "$tar_archive" "${ssh_target}:${remote_root}/modules.tar.gz"
      scp "${scp_opts[@]}" "$env_file" "${ssh_target}:${remote_root}/jumpbox-env.sh"
      rm -f "$env_file" "$tar_archive"
    displayName: "Upload Code and Env to Jumpbox"
    condition: ne(variables['JUMPBOX_VM_ID'], '')
    env:
      ${{ if eq(parameters.cloud, 'azure') }}:
        CLOUD: aks
      ${{ else }}:
        CLOUD: ${{ parameters.cloud }}
      REGION: ${{ parameters.region }}
      CL2_IMAGE: ${{ parameters.engine_input.image }}
      JUMPBOX_HOST: $(JUMPBOX_HOST)
      JUMPBOX_VM_ID: $(JUMPBOX_VM_ID)
      BASTION_NAME: $(BASTION_NAME)
      BASTION_RESOURCE_GROUP: $(BASTION_RESOURCE_GROUP)
      BASTION_TUNNEL_PORT: $(BASTION_TUNNEL_PORT)

  # 3. Execute Benchmark
  - template: /steps/ssh/run-command.yml
    parameters:
      vm_ip: $(JUMPBOX_HOST)
      vm_id: $(JUMPBOX_VM_ID)
      bastion_name: $(BASTION_NAME)
      bastion_resource_group: $(BASTION_RESOURCE_GROUP)
      command: |
        set -e
        tar -xzf '/tmp/telescope-run/run-$(Build.BuildId)/modules.tar.gz' -C '/tmp/telescope-run/run-$(Build.BuildId)'
        pip3 install --user --break-system-packages -r '/tmp/telescope-run/run-$(Build.BuildId)/modules/python/requirements.txt' || pip3 install --user -r '/tmp/telescope-run/run-$(Build.BuildId)/modules/python/requirements.txt'
        source '/tmp/telescope-run/run-$(Build.BuildId)/jumpbox-env.sh'
        PYTHONPATH=${PYTHONPATH:-}:$REMOTE_PYTHON_DIR python3 $PYTHON_SCRIPT_FILE override \
          ${CPU_PER_NODE:-0} ${NODE_COUNT:-0} ${POD_COUNT:-0} \
          $SCALE_UP_TIMEOUT $SCALE_DOWN_TIMEOUT \
          $LOOP_COUNT "${NODE_LABEL_SELECTOR:-""}" "$NODE_SELECTOR" "${CL2_CONFIG_DIR}/overrides.yaml" ${WARMUP_DEPLOYMENT:-false} ${CL2_CONFIG_DIR} --os_type ${OS_TYPE:-linux} --warmup_deployment_template ${WARMUP_DEPLOYMENT_TEMPLATE:-""} --deployment_template ${DEPLOYMENT_TEMPLATE:-""} \
          --pod_cpu_request ${POD_CPU_REQUEST:-0} --pod_memory_request ${POD_MEMORY_REQUEST:-""} --cl2_config_file ${CL2_CONFIG_FILE:-config.yaml} --enable_prometheus ${ENABLE_PROMETHEUS:-False}

        # Note: cluster loader2 newest image is conflicted with service monitors and prometheus
        # to enable prometheus scraping, we need to clean up existing servicemonitors pod first
        # Clean up the conflicting master ServiceMonitor before Prometheus deploys
        if [ "${ENABLE_PROMETHEUS}" = "True" ]; then
          echo "Removing conflicting master ServiceMonitor..."
          kubectl delete servicemonitor master -n monitoring --ignore-not-found=true || true
          sleep 2
        fi

        PYTHONPATH=${PYTHONPATH:-}:$REMOTE_PYTHON_DIR python3 $PYTHON_SCRIPT_FILE execute \
          $CL2_IMAGE $CL2_CONFIG_DIR $CL2_REPORT_DIR "$HOME/.kube/config" $CLOUD --cl2_config_file ${CL2_CONFIG_FILE:-config.yaml} --enable_prometheus ${ENABLE_PROMETHEUS:-False} --scrape_kubelets ${SCRAPE_KUBELETS:-False} --scrape_ksm ${SCRAPE_KSM:-False}

  # 4. Download Results
  - script: |
      set -eo pipefail

      if [ -z "${SSH_KEY_PATH:-}" ] || [ ! -f "$SSH_KEY_PATH" ]; then
        echo "SSH_KEY_PATH not found" >&2; exit 1
      fi

      use_bastion=false
      if [ -n "${BASTION_NAME:-}" ] && [ -n "${BASTION_RESOURCE_GROUP:-}" ] && [ -n "${JUMPBOX_VM_ID:-}" ]; then
        use_bastion=true
      fi

      ssh_opts=(-i "$SSH_KEY_PATH" -o "StrictHostKeyChecking=no" -o "UserKnownHostsFile=/dev/null" -o "ServerAliveInterval=60" -o "ServerAliveCountMax=5")
      # scp uses -P for port (uppercase). Keep a separate opts array so we don't
      # accidentally pass ssh's "-p <port>" into scp.
      scp_opts=("${ssh_opts[@]}")
      local_port="${BASTION_TUNNEL_PORT:-2222}"
      if [ "${use_bastion}" = "true" ]; then
        # Prefer a shared tunnel if one was started earlier in the job.
        if [ -n "${BASTION_TUNNEL_PORT:-}" ] && timeout 1 bash -c "</dev/tcp/127.0.0.1/${local_port}" >/dev/null 2>&1; then
          echo "Reusing existing Bastion tunnel on localhost:${local_port}"
        else
          echo "Starting Bastion tunnel for download..."
          az network bastion tunnel \
            --name "$BASTION_NAME" \
            --resource-group "$BASTION_RESOURCE_GROUP" \
            --target-resource-id "$JUMPBOX_VM_ID" \
            --resource-port 22 \
            --port "$local_port" \
            >/tmp/bastion-tunnel-download-$$.log 2>&1 &
          tunnel_pid=$!
          trap 'kill ${tunnel_pid} 2>/dev/null || true' EXIT
        fi

        for i in $(seq 1 30); do
          if timeout 1 bash -c "</dev/tcp/127.0.0.1/${local_port}" >/dev/null 2>&1; then
            break
          fi
          sleep 1
        done

        ssh_target="${JUMPBOX_USERNAME:-azureuser}@127.0.0.1"
        ssh_opts+=( -p "$local_port" )
        scp_opts+=( -P "$local_port" )
      else
        if [ -z "${JUMPBOX_HOST:-}" ]; then
          echo "JUMPBOX_HOST is not set." >&2
          exit 1
        fi
        ssh_target="${JUMPBOX_USERNAME:-azureuser}@${JUMPBOX_HOST}"
      fi

      remote_report_dir="/tmp/telescope-run/run-$(Build.BuildId)/modules/python/clusterloader2/autoscale/results"
      local_report_dir="$(Pipeline.Workspace)/s/modules/python/clusterloader2/autoscale/results"

      echo "=== Debug Info ==="
      echo "Jumpbox Host: ${JUMPBOX_HOST}"
      echo "SSH Key Path: ${SSH_KEY_PATH}"
      echo "Remote Dir: ${remote_report_dir}"
      echo "Local Dir: ${local_report_dir}"
      echo ""

      echo "=== Testing SSH connection ==="
      ssh -v "${ssh_opts[@]}" "${ssh_target}" "echo 'SSH OK'; ls -la ${remote_report_dir} | head -20" || echo "SSH test failed"
      echo ""

      echo "=== Downloading results ==="
      rm -rf "$local_report_dir"
      mkdir -p "$(dirname "$local_report_dir")"
      scp -v -r "${scp_opts[@]}" "${ssh_target}:${remote_report_dir}" "$(dirname "$local_report_dir")"
    displayName: "Download Results from Jumpbox"
    condition: ne(variables['JUMPBOX_VM_ID'], '')
    env:
      JUMPBOX_HOST: $(JUMPBOX_HOST)
      JUMPBOX_VM_ID: $(JUMPBOX_VM_ID)
      BASTION_NAME: $(BASTION_NAME)
      BASTION_RESOURCE_GROUP: $(BASTION_RESOURCE_GROUP)
      BASTION_TUNNEL_PORT: $(BASTION_TUNNEL_PORT)
