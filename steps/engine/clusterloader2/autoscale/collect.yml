parameters:
- name: cloud
  type: string
  default: ''
- name: engine_input
  type: object
  default: {}
- name: region
  type: string

steps:
- script: |
    kubectl delete --all nodeclaim --ignore-not-found
    kubectl delete --all nodepool --ignore-not-found

    # Get all nodes with the Karpenter nodepool label
    NODES=$(kubectl get nodes -l karpenter.sh/nodepool -o jsonpath='{.items[*].metadata.name}')

    # Loop through each node to extract the instance ID and terminate
    for NODE in $NODES; do
      INSTANCE_ID=$(kubectl get node "$NODE" -o json | jq -r '.spec.providerID' | cut -d '/' -f5)

      # Terminate the EC2 instance
      if [ -n "$INSTANCE_ID" ]; then
        echo "Terminating instance: $INSTANCE_ID"
        #aws ec2 terminate-instances --instance-ids "$INSTANCE_ID"
      else
        echo "No instance ID found for node: $NODE"
      fi
    done
  displayName: "Delete NodeClaim. NodePool and Terminate EC2 Instances"
  condition: always()
- template: /steps/cloud/${{ parameters.cloud }}/collect-cloud-info.yml
  parameters:
    region: ${{ parameters.region }}
- script: |
    set -eo pipefail

    PYTHONPATH=$PYTHONPATH:$(pwd) python3 $PYTHON_SCRIPT_FILE collect $CPU_PER_NODE $NODE_COUNT $POD_COUNT \
      $CL2_REPORT_DIR "$CLOUD_INFO" $RUN_ID $RUN_URL $TEST_RESULTS_FILE
  workingDirectory: modules/python/clusterloader2
  env:
    CLOUD: ${{ parameters.cloud }}
    RUN_URL: $(System.TeamFoundationCollectionUri)$(System.TeamProject)/_build/results?buildId=$(Build.BuildId)&view=logs&j=$(System.JobId)
    PYTHON_SCRIPT_FILE: $(Pipeline.Workspace)/s/modules/python/clusterloader2/autoscale/autoscale.py
    CL2_REPORT_DIR: $(Pipeline.Workspace)/s/modules/python/clusterloader2/autoscale/results
  displayName: "Collect Results"
